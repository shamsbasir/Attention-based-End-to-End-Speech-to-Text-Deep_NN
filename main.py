# -*- coding: utf-8 -*-
# most of the codes are from the Recitation and Piazza comments that the TAs and others have made
# 
"""Pro_my_custom_model_hw4p2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jgKQvNMMVEffaYuEUiWFZjFoc_RXZE8o
"""

reset = False

if reset:
  # mount drive, install kaggle, download the data, and unzip
  from google.colab import drive
  drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
if reset:
  !pip install -q kaggle
  !pip install -q --upgrade kaggle
  !pip install -q python-levenshtein
#   %rm -rf /root/.kaggle/
#   %mkdir /root/.kaggle
#   %cp /content/gdrive/My\ Drive/hw4_p2/kaggle.json  /root/.kaggle/

# Commented out IPython magic to ensure Python compatibility.
if reset:
#   %cd /content/ 
  !kaggle competitions download -q -c 11-785-fall-20-homework-4-part-2   
  !unzip -q \*.zip

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
import torch
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils as utils
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
import random
import time
import warnings
from tqdm import tqdm
warnings.filterwarnings("ignore", category=UserWarning)
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
import Levenshtein

# https://colab.research.google.com/notebooks/pro.ipynb#scrollTo=23TOba33L4qf
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

# https://colab.research.google.com/notebooks/pro.ipynb#scrollTo=V1G82GuO-tez
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('To enable a high-RAM runtime, select the Runtime > "Change runtime type"')
  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')
  print('re-execute this cell.')
else:
  print('You are using a high-RAM runtime!')

def transform_letter_to_index(transcript):
    '''
    :param transcript :(N, ) Transcripts are the text input
    :param letter_list: Letter list defined above
    :return letter_to_index_list: Returns a list for all the transcript sentence to index
    '''
    letter_to_index_list = []
    for sent in transcript:
        line = [letter2index['<sos>']]
        for word in sent:
            
            w = word.decode('utf-8')
            for c in w:
                line.append(letter2index[c])
            line.append(letter2index[' ']) # space between words 
        line.pop() # last word ends without space 
        line.append(letter2index['<eos>']) #end of the last word 
        letter_to_index_list.append(line)
    return letter_to_index_list

def transform_index_to_letter(indexed_Text):
  end_indices = [letter2index['<eos>'], letter2index['<pad>']]
  index_to_letter_list = []
  for line in indexed_Text:
      sent = ""
      for i in line:
          # Reached the end of the sentence
          if i in end_indices:
              break
          else:
              sent += index2letter[i]
      index_to_letter_list.append(sent)
  return index_to_letter_list

vocab = ['<pad>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \
               'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-', "'", '.', '_', '+', ' ','<sos>','<eos>']

letter2index = dict(zip(vocab, range(len(vocab))))
index2letter = dict(zip(range(len(vocab)),vocab))

class Speech2TextDataset(Dataset):
    '''
    Dataset class for the speech to text data, this may need some tweaking in the
    getitem method as your implementation in the collate function may be different from
    ours.
    '''
    def __init__(self, speech, text):
        self.dataX = speech
        self.dataY = text

    def __len__(self):
        return self.dataX.shape[0]

    def __getitem__(self, index):
        if self.dataY == None: # test scenario
            return torch.tensor(self.dataX[index].astype(np.float32))
        else:
            return torch.tensor(self.dataX[index].astype(np.float32)), torch.tensor(self.dataY[index])

def collate_train(batch):
    ### Return the padded speech and text data, and the length of utterance and transcript ###
    utterances = []
    targets = []

    utterances_len = []
    targets_lens = []
    
    for b in range(len(batch)):
        utterances.append(torch.tensor(batch[b][0]))
        utterances_len.append(len(batch[b][0]))

        targets.append(torch.tensor(batch[b][1][1:])) # don't send <sos> we don't predict that 
        targets_lens.append(len(batch[b][1])-1) #  sentence <eos>


    utterances = pad_sequence(utterances, batch_first=True) # dim (B, T, C) 
    targets = pad_sequence(targets, batch_first=True,padding_value=letter2index['<pad>'])

    utterances_len = torch.tensor(utterances_len)
    targets_lens = torch.tensor(targets_lens)

    return utterances, targets, utterances_len, targets_lens


def collate_test(batch):
    ### Return padded speech and length of utterance ###
    utterances = []
    utterances_len = []

    for b in range(len(batch)):
        utterances.append(torch.tensor(batch[b]))
        utterances_len.append(len(batch[b]))

    utterances = pad_sequence(utterances, batch_first=True)
    
    utterances_len = torch.tensor(utterances_len)

    return utterances, utterances_len

def plot_grad_flow(named_parameters):    
    '''Plots the gradients flowing through different layers in the net during training.    
       Can be used for checking for possible gradient vanishing / exploding problems.        
       Usage: Plug this function in Trainer class after loss.backwards() as     
       "plot_grad_flow(self.model.named_parameters())" to visualize the gradient flow
    '''    
    ave_grads = []    
    max_grads= []    
    layers = []    
    for n, p in named_parameters:        
        if(p.requires_grad) and ("bias" not in n):            
            if(p is not None):                
                layers.append(n)                
                ave_grads.append(p.grad.abs().mean())                
                max_grads.append(p.grad.abs().max())
    plt.clf()
    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color="c")    
    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color="b")    
    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color="k" )    
    plt.xticks(range(0,len(ave_grads), 1), layers, rotation="vertical")    
    plt.xlim(left=0, right=len(ave_grads))    
    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions    
    plt.xlabel("Layers")
    plt.ylabel("average gradient")    
    plt.title("Gradient flow")    
    #plt.tight_layout()    
    plt.grid(True)    
    plt.legend([Line2D([0], [0], color="c", lw=4),
                Line2D([0], [0], color="b", lw=4),
                Line2D([0], [0], color="k", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])    
    return plt

"""# Locked_Dropout"""

# Original implementation:
# https://github.com/salesforce/awd-lstm-lm/blob/master/locked_dropout.py

class LockedDropout(nn.Module):
    """ LockedDropout applies the same dropout mask to every time step.

    Args:
        p (float): Probability of an element in the dropout mask to be zeroed.
    """

    def __init__(self,p):
        super().__init__()
        self.p = p
    
    
    def forward(self, x):
        """
        Args:
            x (:class:`torch.FloatTensor` [batch size, sequence length, rnn hidden size]): Input to
                apply dropout too.
        """
     
        
        if not self.training or not self.p:
            return x
        x = x.clone()
    
        
        mask = x.new_empty(x.size(0),1, x.size(2), requires_grad=False).bernoulli_(1 - self.p)
        mask = mask.div_(1 - self.p)
        mask = mask.expand_as(x)
        return x * mask

"""# Pyramidal BLSTM"""

class pBLSTM(nn.Module):
    '''
    Pyramidal BiLSTM
    The length of utterance (speech input) can be hundereds to thousands of frames long.
    The Paper reports that a direct LSTM implementation as Encoder resulted in slow convergence,
    and inferior results even after extensive training.
    The major reason is inability of AttendAndSpell operation to extract relevant information
    from a large number of input steps.
    '''
    def __init__(self, input_dim, hidden_dim,dropout_rate):
        super(pBLSTM, self).__init__()

        self.dropout = LockedDropout(dropout_rate)

        self.blstm   = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True)

    def forward(self, x):
        '''
        :param x :(N, T) input to the pBLSTM is packed
        :return output: (N, T, H) encoded sequence from pyramidal Bi-LSTM
        '''

        x_padded, x_lens = pad_packed_sequence(x, batch_first=True) # padded 

        # chop off the extra piece
        x_padded = x_padded[:, :(x_padded.size(1) // 2) * 2, :] # (B, T, dim)
        
        # reshape to (B, T/2, dim*2)
        x_reshaped = x_padded.reshape(x_padded.size(0), x_padded.size(1) // 2, x_padded.size(2) * 2)


        # halve the time lengths
        x_lens = x_lens // 2
        # pack it for the LSTM

        # x_padded : (batch, time length, feature dim)
        x_reshaped = self.dropout(x_reshaped)
        # locked dropout

        x_packed = pack_padded_sequence(x_reshaped, lengths=x_lens.cpu(), batch_first=True, enforce_sorted=False)

        # run through LSTM
        out, hidden = self.blstm(x_packed)

        return out,hidden

"""# Encoder"""

class Encoder(nn.Module):
    '''
    Encoder takes the utterances as inputs and returns the key and value.
    Key and value are nothing but simple projections of the output from pBLSTM network.
    '''
    def __init__(self, input_dim, hidden_dim, n_layers, dropout_rate, value_size=128,key_size=128):
        super(Encoder, self).__init__()

        self.lstm   = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True)

        self.pBLSTM_block = nn.ModuleList([pBLSTM(hidden_dim*4, hidden_dim, dropout_rate) 
                                           for _ in range(n_layers-1)])


        self.key_network = nn.Linear(hidden_dim*2, value_size)

        self.value_network = nn.Linear(hidden_dim*2, key_size)

    def forward(self, x, lens):

        rnn_inp = pack_padded_sequence(x, lengths=lens.cpu(), batch_first=True, enforce_sorted=False)

        outputs, _ = self.lstm(rnn_inp) 

        # outputs is packed 
        for plstm in self.pBLSTM_block:

             outputs, hidden = plstm(outputs)  

        linear_input, encoder_lens = pad_packed_sequence(outputs, batch_first=True)

        keys = self.key_network(linear_input)
        
        value = self.value_network(linear_input)
      
        return keys, value, encoder_lens, hidden

"""# Attention """

class Attention(nn.Module):
    '''
    Attention is calculated using key, value and query from Encoder and decoder.
    Below are the set of operations you need to perform for computing attention:
        energy = bmm(key, query)
        attention = softmax(energy)
        context = bmm(attention, value)
    '''
    def __init__(self):
        super(Attention, self).__init__()

    def forward(self, query, key, value, lens):
        '''
        :param query :(N, context_size) Query is the output of LSTMCell from Decoder
        :param key: (N, T_max, key_size) Key Projection from Encoder per time step
        :param value: (N, T_max, value_size) Value Projection from Encoder per time step
        :param lens: (N, T) Length of key and value, used for binary masking
        :return output: Attended Context
        :return attention: Attention mask that can be plotted
        '''
        #scale = torch.sqrt(torch.FloatTensor([key.size(2)])).to(DEVICE)

        energy = (torch.bmm(key, query.unsqueeze(2)).squeeze(2)) #/scale 
        # (N, T_max, key_size) * (N, context_size, 1) = (N, T_max, 1) -> (N, T_max)

       
        mask = torch.arange(key.size(1)).unsqueeze(0) >= lens.unsqueeze(1) 
        # (1, T) >= (B, 1) -> (N, T_max)

        mask = mask.to(DEVICE)
        energy.masked_fill_(mask, -1e9)
         # (N, T_max)

        attention = nn.functional.softmax(energy, dim=1) 
        # (N, T_max)

        output = torch.bmm(attention.unsqueeze(1), value).squeeze(1) 
        # (N, T_max)

        return output, attention

"""# Decoder"""

class Decoder(nn.Module):
    '''
    As mentioned in a previous recitation, each forward call of decoder deals with just one time step,
    thus we use LSTMCell instead of LSLTM here.
    The output from the second LSTMCell can be used as query here for attention module.
    In place of value that we get from the attention, this can be replace by context we get from the attention.
    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.
    '''
    def __init__(self, vocab_size, decoder_hidden_dim, embed_dim, value_size=128, key_size=128, isAttended=False):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=letter2index['<pad>'])

        self.lstm1 = nn.LSTMCell(input_size=embed_dim + value_size, hidden_size=decoder_hidden_dim)
        self.lstm2 = nn.LSTMCell(input_size=decoder_hidden_dim, hidden_size=key_size)

        self.isAttended = isAttended
        if (isAttended == True):
            self.attention = Attention()

        self.character_prob = nn.Linear(key_size + value_size, vocab_size)

        # weight tying 
        
        #self.character_prob.weight = self.embedding.weight
        self.hidden_dim = decoder_hidden_dim
        self.vocab_size = vocab_size


    def forward(self, key, values, encoder_lens, hidden, text=None, isTrain=True, teacherForcingRate = 2.0):
        '''
        :param key :(N, T, key_size) Output of the Encoder Key projection layer
        :param values: (N, T, value_size) Output of the Encoder Value projection layer
        :param text: (N, text_len) Batch input of text with text_length
        :param isTrain: Train or eval mode
        :return predictions: Returns the character perdiction probability
        '''
        batch_size = key.shape[0]

        if (isTrain == True):
          #print("training")

          max_len =  text.shape[1]

          embeddings = self.embedding(text)
        else:
          #print('validating')
          max_len = 600

        predictions = []
        
        # Hidden states: (2, batch_size, hidden_size) -> (batch_size, 2*hidden_size)
        hidden = tuple(st.transpose(0, 1).reshape(batch_size, -1) for st in hidden)
        hidden_states = [hidden, None]
        #hidden_states = [None,None]
        context = values[:,0,:]

        prediction = torch.zeros(batch_size, self.vocab_size).to(DEVICE)
        prediction[:, letter2index['<sos>']] = 1
        attentionPlot = []

        for i in range(max_len):

          if i == 0 :

            char_embed = self.embedding(prediction.argmax(dim=-1))
            #print("i = 0")

          else:

            if (isTrain):

              teacher_force = random.random() > teacherForcingRate 

              char_embed = embeddings[:, i-1, :] if teacher_force else self.embedding(prediction.argmax(dim=-1))

            else:

              char_embed = self.embedding(prediction.argmax(dim=-1))


          # Input to decoder is the concatenated char embedding and attention context vector
          inp = torch.cat([char_embed, context], dim=1)
          hidden_states[0] = self.lstm1(inp, hidden_states[0])
  
          inp_2 = hidden_states[0][0]
          hidden_states[1] = self.lstm2(inp_2, hidden_states[1]) 
          # output (h_1, c_1)
  
          ### Compute attention from the output of the second LSTM Cell ###
          output = hidden_states[1][0]

          # context calculation for the output
          context, _ = self.attention(output, key, values, encoder_lens)
  
          prediction = self.character_prob(torch.cat([output,context],dim=1))

          #print("prediction : ",prediction.shape)
          predictions.append(prediction.unsqueeze(1))

        return torch.cat(predictions, dim=1)

    def BeamSearch(self, key,values, encoder_lens, hidden, beam_width):
  
        max_len = 600
        predictions = []
        Path = [ {'current_path': torch.LongTensor([letter2index['<sos>']]).to(DEVICE),
                          'score': 0,
                          'hidden_states':[None, None],
                          'context': values[:,0,:],
                          'path':[]} ]
  
        so_far_path =[]
        for t in range(max_len):
          tmp_path = []
          for path in Path:       
            #print("t = :",t)
            # Input to decoder is the concatenated char embedding and attention context vector
            char_embed = self.embedding(path['current_path'])
            #print("path['context']", path['context'].shape)
            #print('char_embed:',char_embed.shape)
            inp = torch.cat([char_embed, path['context']], dim=1)
            hidden_states = path['hidden_states']
            hidden_states[0] = self.lstm1(inp, hidden_states[0])
            inp_2 = hidden_states[0][0]
            hidden_states[1] = self.lstm2(inp_2, hidden_states[1]) 
  
            # output (h_1, c_1)
            ### Compute attention from the output of the second LSTM Cell ###
            output = hidden_states[1][0]
            # context calculation for the output
            context, _ = self.attention(output, key, values, encoder_lens)
            outprobs = self.character_prob(torch.cat([output,context],dim=1))
            # pick to k 
            top_k_score, top_k_indices = torch.topk(F.log_softmax(outprobs, dim = 1), beam_width, dim = 1)
  
            for bidk in range(beam_width):
              tmp_dict = {}
              tmp_dict['score'] = path['score'] + top_k_score[:,bidk]
              tmp_dict['current_path'] = top_k_indices[:, bidk]
              tmp_dict['path'] = path['path'] + [top_k_indices[:, bidk]]
              tmp_dict['hidden_states'] = hidden_states[:]
              tmp_dict['context'] = context
              tmp_path.append(tmp_dict)
  
          # sorted(iterable, key=None, reverse=False)
          # lambda arguments : expression

          tmp_path = sorted(tmp_path, key = lambda pth : pth['score'], reverse = True)[:beam_width]
  
          if t == max_len - 1: # end of the sentences 
            for path in tmp_path:
              path['current_path'] = letter2index['<eos>']
              #path['path'] = path['path'] + [letter2index['<eos>']]
  
  
          Path = []
          for path in tmp_path:
           
            if path['current_path'] == letter2index['<eos>']:
              normalization = len(path['path'])
              path['score'] /= normalization
              so_far_path.append(path)
          
            else:
              Path.append(path)
  
          if len(Path) == 0:
            break
        best_path = sorted(so_far_path, key = lambda p : p['score'], reverse = True)[0]
  
        return best_path['path'][:-1]

class Seq2Seq(nn.Module):
    '''
    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.
    This is simply a wrapper "model" for your encoder and decoder.
    '''
    def __init__(self, input_dim, vocab_size, encoder_hidden_dim=256, 
                 encoder_n_layers=3, decoder_hidden_dim=512,
                 embed_dim=256, value_size=128, key_size=128, 
                 dropout_rate=0.2, isAttended=True):
      
        super(Seq2Seq, self).__init__()
        self.encoder = Encoder(input_dim, encoder_hidden_dim, encoder_n_layers, dropout_rate,value_size,key_size) 

        self.decoder = Decoder(vocab_size, decoder_hidden_dim, embed_dim, value_size, key_size, isAttended)

    def forward(self,speech_input, speech_len, text_input, isTrain,teach_force = 2.0):

        key, value, encoder_lens, hidden = self.encoder(speech_input, speech_len)

        if (isTrain == True):

            predictions = self.decoder(key, value, encoder_lens, hidden, text_input,isTrain,teach_force)
        else:
            predictions = self.decoder(key, value, encoder_lens, hidden, text=None, isTrain=False)

        return predictions

    def beam_decoder(self,speech_input, speech_len,beam_width):
        # encoder
        key, value, encoder_lens, hidden = self.encoder(speech_input, speech_len)
        # Beam decoding
        predictions = self.decoder.BeamSearch(key,value, encoder_lens, hidden, beam_width)
        return predictions

def Lev_distance(preds, targets):
    res = 0.0
    for pred, target in zip(preds, targets):
        dist = Levenshtein.distance(pred, target)
        res += dist
    return res

def train(model, train_loader, criterion, optimizer, teach_force, print2displayat=50):
    model.train()
    model.to(DEVICE)

    runningLoss = 0.0
    runningPerplex = 0.0

    # 1) Iterate through your loader
    for batch_idx, (data, target, dataLens, targetLens) in enumerate(tqdm(train_loader)):

        # 2) Use torch.autograd.set_detect_anomaly(True) to get notices about gradient explosion
        with torch.autograd.set_detect_anomaly(False): # close to 

            # 3) Set the inputs to the device.
            data, target, targetLens = data.to(DEVICE), target.to(DEVICE), targetLens.to(DEVICE)
            optimizer.zero_grad()
      
            # 4) Pass your inputs, and length of speech into the model.
            predictions = model(speech_input=data, speech_len=dataLens, text_input=target, isTrain=True,  teach_force=teach_force)
            # (batch size, max seq len, vocab size)
            
            predictions = predictions.permute(0, 2, 1)
            # (batch size, vocab size, max seq len)

            loss = criterion(predictions, target)

            currLoss = loss.item()
            currPerplex = torch.exp(loss).item()

            runningLoss += currLoss
            runningPerplex += currPerplex
           
            # 9) Run the backward pass on the masked loss.
            loss.backward()
 
            torch.nn.utils.clip_grad_norm(model.parameters(), 2)

            # 11) Take a step with your optimizer
            optimizer.step()


            # 13) Optionally print the training loss after every N batches

            if (batch_idx+1) % print2displayat == 0:
                print("Batch: {}\tAvg_Loss: {:.5f}\tAvg_Preplex:{:.5f} ".format(batch_idx+1, runningLoss/print2displayat, runningPerplex/print2displayat))
                runningLoss = 0.0
                runningPerplex = 0.0

            del data
            del target
            del targetLens
            torch.cuda.empty_cache()

def val(model, dev_loader, criterion, print2displayat=20):

    model.eval()
    model.to(DEVICE)

    total = 0.0
    dist = 0.0

    for batch_idx, (data, target, dataLens, targetLens) in enumerate(tqdm(dev_loader)):
      
        data, target, targetLens = data.to(DEVICE), target.to(DEVICE), targetLens.to(DEVICE)
        
        predictions = model(speech_input=data, speech_len=dataLens, text_input=None, isTrain=False)

        predicted_text = transform_index_to_letter(predictions.argmax(-1).detach().cpu().numpy())
        target_Text = transform_index_to_letter(target.detach().cpu().numpy())
        
        dist += Lev_distance(predicted_text, target_Text)
        total += len(predicted_text)
        
        if (batch_idx+1) % print2displayat == 0 :
            print("-"*30)
            print("Prediction :\n{}\n Original text:\n{}\n".format(predicted_text[0], target_Text[0]))
            print("-"*30)
            
        del data
        del target
        del targetLens
        torch.cuda.empty_cache()

    end = time.time()
    print("\t distance: {:.3f} ".format(dist/total))
    return dist/total

def inference(model, test_loader, beam_width):
  transcripted = []
  with torch.no_grad():
    model.eval()
    for batch_idx, (data, dataLens) in enumerate(tqdm(test_loader)):
      data = data.to(DEVICE)

      predicted_labels = model.beam_decoder(data, dataLens, beam_width)

      tmp_transcripted = ''
      for i in predicted_labels:
          tmp_transcripted += vocab[i]
      transcripted.append(tmp_transcripted)


      del data
      torch.cuda.empty_cache()
  return transcripted

# %%
PATH = {
    'checkpoint':"/content/gdrive/My Drive/hw4_p2/checkpoint/",
    "saved_model": "/content/gdrive/My Drive/hw4_p2/checkpoint/Pro_Letest_model.txt",
}

speech_train = np.load('/content/hw4p2/train.npy', allow_pickle=True, encoding='bytes')
speech_valid = np.load('/content/hw4p2/dev.npy', allow_pickle=True, encoding='bytes')
speech_test = np.load('/content/hw4p2/test.npy', allow_pickle=True, encoding='bytes')
transcript_train = np.load('/content/hw4p2/train_transcripts.npy', allow_pickle=True,encoding='bytes')
transcript_valid = np.load('/content/hw4p2/dev_transcripts.npy', allow_pickle=True,encoding='bytes')

character_text_train = transform_letter_to_index(transcript_train)
character_text_dev = transform_letter_to_index(transcript_valid)

# data Loaders
train_dataset = Speech2TextDataset(speech_train, character_text_train)
dev_dataset   = Speech2TextDataset(speech_valid, character_text_dev)
test_dataset  = Speech2TextDataset(speech_test, None)

train_loader = DataLoader(train_dataset, batch_size= 256, shuffle=True, collate_fn=collate_train) 
dev_loader = DataLoader(dev_dataset, batch_size= 64, shuffle=False,  collate_fn=collate_train) 
test_loader = DataLoader(test_dataset, batch_size= 1, shuffle=False,  collate_fn=collate_test)

def init_weights(m):
    for name, param in m.named_parameters():
        if 'weight' in name:
            nn.init.normal_(param.data, mean=0, std=0.01)
        else:
            nn.init.constant_(param.data, 0)

# Model 
model = Seq2Seq(input_dim=40, vocab_size=len(vocab),
                encoder_hidden_dim=256,
                decoder_hidden_dim=512,
                embed_dim=256,
                value_size=128,
                key_size=128,
                dropout_rate = 0.25,
                isAttended=True)


#ckpt = torch.load(PATH["saved_model"])
#model.load_state_dict(ckpt["model_state_dict"])
#model.apply(init_weights)
model.to(DEVICE)
model.apply(init_weights)
print(model)

# Nonte: Move the model first and then call the optimizer because the parameters
# on the GPU will be different 
# Make sure the parameters for the optimizer are on the same device 

optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss(ignore_index=letter2index['<pad>'])
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=2, verbose=True, threshold=1e-2)
#optimizer.load_state_dict(ckpt['optimizer_state_dict'])

def stopwatch(start_time, end_time):
    time = end_time - start_time
    minutes = int(time / 60)
    seconds = int(time - (minutes * 60))
    return minutes, seconds

def teacher_force_rate(epoch):
    '''
      Args : 
      
        input: 
            epoch: (int) number of epochs 
        output: 
            teacher_force : (float) probability of teacher forcing
    '''
    
    if epoch <= 5:
        teacher_force = 0.10
        return teacher_force
    
    elif epoch <= 10:
        teacher_force = 0.20
        return teacher_force
    
    elif epoch <= 15:
        teacher_force = 0.30
        return teacher_force

    else :
        teacher_force = 0.40
        return teacher_force

def teacher_forcing_rate(teacher_rate_old):
    '''
      Args : 
      
        input: 
            epoch: (int) number of epochs 
        output: 
            teacher_force : (float) probability of teacher forcing
    '''
    return teacher_rate_old + 0.01

teach_force = 0.01
best_editDist = float('inf')
for epoch in range(1,40):
  
    torch.cuda.empty_cache()
    # Print current learnng rate
    for prarm_group in optimizer.param_groups:
        print("Current lr: \t{}".format(prarm_group["lr"]))
    
    teach_force = teacher_forcing_rate(teach_force)
    print("Current tf: {}".format(teach_force))

    # Train
    start_time = time.time()
    train(model, train_loader, criterion, optimizer, teach_force, 64)
    
    # Evaluate
    Lev_dist = val(model, dev_loader, criterion, 32)
    scheduler.step(Lev_dist)


    end_time = time.time()

    epoch_mins, epoch_secs = stopwatch(start_time, end_time)

    print(f'Epoch: {epoch:02} | Time: {epoch_mins}m {epoch_secs}s')
    print('*-'*20)
    
    if (Lev_dist < best_editDist):
      best_editDist = Lev_dist
      print("*** Saving ***")
      path = "{}WeightTying_Latest_model.txt".format(PATH["checkpoint"])
      torch.save({
        'epoch':epoch,
        'model_state_dict':model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()}, path)

# Inference 
ckpt = torch.load(PATH["saved_model"])
model.load_state_dict(ckpt["model_state_dict"])
model.to(DEVICE)
result = inference(model, test_loader, beam_width=4)
dataframe = pd.DataFrame({'id':[i for i in range(len(result))],'label':result})
dataframe.to_csv("submission.csv", index=False)
!kaggle competitions submit -c 11-785-fall-20-homework-4-part-2 -f submission.csv -m "Message"

